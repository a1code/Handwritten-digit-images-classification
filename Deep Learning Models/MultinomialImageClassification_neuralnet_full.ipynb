{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "c4HO0",
      "launcher_item_id": "lSYZM"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "MultinomialImageClassification_neuralnet_full.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERl7-jmJMce_"
      },
      "source": [
        "Based on my learnings from the Deep Learning Specialization offered by [deeplearning.ai](https://www.coursera.org/specializations/deep-learning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcu_XUoZnjQI"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from urllib import request\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL5B5JtrtS4E"
      },
      "source": [
        "# this function initializes model parameters with zeros \n",
        "\n",
        "def initialize_parameters_zeros(layers_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- list containing the size of each layer.\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- dictionary containing parameters for all layers \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
        "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
        "                    ...\n",
        "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
        "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    parameters = {}\n",
        "    L = len(layers_dims)            # number of layers in the network\n",
        "    \n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))\n",
        "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH0wrU8gZEDZ"
      },
      "source": [
        "# this function initializes model parameters randomly\n",
        "\n",
        "def initialize_parameters_random(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- list containing the size of each layer.\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- dictionary containing parameters for all layers \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
        "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
        "                    ...\n",
        "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
        "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGyNj9CCsolu"
      },
      "source": [
        "# this function initializes model parameters using He initialization (He-et-al, 2015)\n",
        "\n",
        "def initialize_parameters_he(layers_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- list containing the size of each layer.\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- dictionary containing parameters for all layers \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
        "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
        "                    ...\n",
        "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
        "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    parameters = {}\n",
        "    L = len(layers_dims) - 1\n",
        "     \n",
        "    for l in range(1, L + 1):\n",
        "        parameters['W' + str(l)] = np.random.randn(\n",
        "            layers_dims[l], layers_dims[l-1]) * np.sqrt(2.0 / layers_dims[l-1])\n",
        "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM988bp6ZEDt"
      },
      "source": [
        "# this function does linear computation for a given layer during forward propagation\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer, shape: (size of previous layer, number of examples)\n",
        "    W -- weights matrix, shape: (size of current layer, size of previous layer)\n",
        "    b -- bias vector, shape: (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- tuple containing \"A\", \"W\" and \"b\" ; used for computations during backprop\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = np.dot(W, A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsvVzY7OhPTE"
      },
      "source": [
        "# activation functions for use during forward propagation\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the RELU function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter, same shape as Z\n",
        "    cache -- dictionary containing \"Z\" ; used for computations during backprop\n",
        "    \"\"\"\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "    \n",
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implements the sigmoid activation in numpy\n",
        "    \n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of sigmoid(z), same shape as Z\n",
        "    cache -- dictionary containing \"Z\" ; used for computations during backprop\n",
        "    \"\"\"\n",
        "    \n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def softmax(Z):\n",
        "    \"\"\"Compute softmax values for each sets of scores in Z.\n",
        "    For stability, we can multiply both the numerator and denominator with a constant c.\n",
        "    A popular choice of the log(c) constant is âˆ’max(z)\n",
        "\n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of softmax(z)\n",
        "    cache -- dictionary containing \"Z\" ; used for computations during backprop\n",
        "    \"\"\"\n",
        "\n",
        "    expZ = np.exp(Z - np.max(Z))\n",
        "    A = expZ / expZ.sum(axis=0, keepdims=True)\n",
        "    \n",
        "    cache = Z\n",
        "    return A, cache"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s689cxOkZEEB"
      },
      "source": [
        "# this function implements forward propagation for a given layer,\n",
        "# with or without dropout\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation, keep_prob=1.0):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer, shape: (size of previous layer, number of examples)\n",
        "    W -- weights matrix, shape: (size of current layer, size of previous layer)\n",
        "    b -- bias vector, shape: (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer: \"sigmoid\", \"relu\" or \"softmax\"\n",
        "    keep_prob -- probability of keeping a hidden unit active during drop-out, scalar\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "             used for computations during backprop\n",
        "    \"\"\"\n",
        "    \n",
        "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "\n",
        "    if activation == \"sigmoid\":        \n",
        "        A, activation_cache = sigmoid(Z)\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "        A, activation_cache = relu(Z)\n",
        "\n",
        "    elif activation == \"softmax\":\n",
        "        A, activation_cache = softmax(Z)\n",
        "           \n",
        "    if keep_prob < 1:\n",
        "        # initialize matrix D\n",
        "        D = np.random.rand(A.shape[0], A.shape[1])   \n",
        "        # convert entries of D to 0 or 1 (using keep_prob as the threshold)\n",
        "        D = (D < keep_prob).astype(int)       \n",
        "        # shut down some hidden units of A\n",
        "        A = np.multiply(A, D)                           \n",
        "        A = A / keep_prob\n",
        "        activation_cache = (activation_cache, D)\n",
        "        \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "    \n",
        "    return A, cache"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GbYG8x7ZEEa"
      },
      "source": [
        "# this function implements forward propagation through all the layers,\n",
        "# with or without dropout\n",
        "\n",
        "def L_model_forward(X, parameters, keep_prob=1.0):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX layers\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, shape: (input size, number of examples)\n",
        "    parameters -- output of model parameter initialization\n",
        "    keep_prob -- probability of keeping a hidden unit active during drop-out, scalar\n",
        "\n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing every cache of linear_activation_forward();\n",
        "    there are L of them, indexed from 0 to L-1\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, \n",
        "                                             parameters['W' + str(l)], \n",
        "                                             parameters['b' + str(l)], \n",
        "                                             \"relu\", \n",
        "                                             keep_prob)\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # Implement LINEAR -> SOFTMAX. Add \"cache\" to the \"caches\" list\n",
        "    AL, cache = linear_activation_forward(A,\n",
        "                                          parameters['W' + str(L)], \n",
        "                                          parameters['b' + str(L)], \n",
        "                                          \"softmax\")\n",
        "    caches.append(cache)\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upvz0YmSZEEt"
      },
      "source": [
        "# this function computes the cross-entropy cost\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function for multinomial classification\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to label predictions, \n",
        "    shape: (number of classes, number of examples)\n",
        "    Y -- one-hot encoded \"true\" label vector, \n",
        "    shape: (number of classes, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # compute cost\n",
        "    cost = (-1.0 / m) * np.nansum(Y * np.log(AL)) \n",
        "    \n",
        "    cost = np.squeeze(cost)      \n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZtrmKPNA9px"
      },
      "source": [
        "# this function computes cost when the model is L2 regularized\n",
        "\n",
        "def compute_cost_with_regularization(AL, Y, parameters, lambd):\n",
        "    \"\"\"\n",
        "    Implement the cost function with L2 regularization\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to label predictions, \n",
        "    shape: (number of classes, number of examples)\n",
        "    Y -- one-hot encoded \"true\" label vector, \n",
        "    shape: (number of classes, number of examples)\n",
        "    parameters -- dictionary containing model parameters\n",
        "    lambd -- regularization hyperparameter, scalar\n",
        "    \n",
        "    Returns:\n",
        "    cost - value of the regularized loss function for multinomial classification\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2    # number of layers in the neural network\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    cross_entropy_cost = compute_cost(AL, Y) # cross-entropy part of the cost\n",
        "\n",
        "    # regularization cost\n",
        "    param_sum = 0\n",
        "    for l in range(1, L):\n",
        "      param_sum += np.sum(np.square(parameters['W' + str(l)]))\n",
        "\n",
        "    param_sum += np.sum(np.square(parameters['W' + str(L)]))\n",
        "    L2_regularization_cost = (lambd / (2.0*m)) * param_sum\n",
        "    \n",
        "    # total cost\n",
        "    cost = cross_entropy_cost + L2_regularization_cost\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlXHLzycZEFD"
      },
      "source": [
        "# this function does linear computation for a given layer during backward propagation\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1),\n",
        "    same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), \n",
        "    same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), \n",
        "    same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1.0/m) * np.dot(dZ, A_prev.T)\n",
        "    db = (1.0/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlWUW0SOhX69"
      },
      "source": [
        "# gradients of activation functions for use during backward propagation\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    Z -- linear computation from forward propagation through the layer\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    dZ = np.array(dA, copy=True) # convert dZ to a correct object.\n",
        "    \n",
        "    # When Z <= 0, set dZ to 0\n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "\n",
        "def sigmoid_backward(dA, Z):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    Z -- linear computation from forward propagation through the layer\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g_7X8yGZEFP"
      },
      "source": [
        "# this function implements backward propagation for a given non-output layer,\n",
        "# with or without dropout\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation, keep_prob=1.0):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION for non-output layers.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of (linear_cache, activation_cache) from forward propagation through the layer\n",
        "    activation -- the activation to be used in this layer: \"sigmoid\" or \"relu\" \n",
        "    keep_prob -- probability of keeping a hidden unit active during drop-out, scalar\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), \n",
        "    same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if keep_prob < 1:\n",
        "        (Z, D) = activation_cache\n",
        "        # Apply mask D to shut down the same hidden units as during the forward propagation\n",
        "        dA = np.multiply(dA, D)\n",
        "        # Scale the value of hidden units that haven't been shut down  \n",
        "        dA = dA / keep_prob \n",
        "    else:\n",
        "        Z = activation_cache\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, Z)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, Z)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)     \n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkdZfOhEZEFs"
      },
      "source": [
        "# this function implements backward propagation through all the layers,\n",
        "# with or without dropout\n",
        "\n",
        "def L_model_backward(AL, Y, caches, keep_prob=1.0):\n",
        "    \"\"\"\n",
        "    Implement backward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX layers\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to label predictions, \n",
        "    shape: (number of classes, number of examples)\n",
        "    Y -- one-hot encoded \"true\" label vector, \n",
        "    shape: (number of classes, number of examples)\n",
        "    caches -- list of caches containing every cache of linear_activation_forward();\n",
        "    there are L of them, indexed from 0 to L-1\n",
        "    keep_prob -- probability of keeping a hidden unit active during drop-out, scalar\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) \n",
        "    \n",
        "    # Lth layer (SOFTMAX -> LINEAR) gradients\n",
        "    linear_cache, activation_cache = caches[L-1]\n",
        "    A_prev, WL, bL = linear_cache\n",
        "    \n",
        "    dZL = AL - Y\n",
        "    dWL = 1./m * np.dot(dZL, A_prev.T)\n",
        "    dbL = 1./m * np.sum(dZL, axis=1, keepdims = True)\n",
        "    dA_prev_temp = np.dot(WL.T, dZL)\n",
        "\n",
        "    grads[\"dW\" + str(L)] = dWL\n",
        "    grads[\"db\" + str(L)] = dbL\n",
        "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
        "\n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)],\n",
        "                                                                    current_cache,\n",
        "                                                                    \"relu\",\n",
        "                                                                    keep_prob)\n",
        "        \n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "        \n",
        "    return grads"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GE1idYao21A"
      },
      "source": [
        "# this function implements backward propagation through all the layers,\n",
        "# with L2 regularization\n",
        "\n",
        "def L_model_backward_with_regularization(AL, Y, caches, lambd):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to label predictions, \n",
        "    shape: (number of classes, number of examples)\n",
        "    Y -- one-hot encoded \"true\" label vector, \n",
        "    shape: (number of classes, number of examples)\n",
        "    caches -- list of caches containing every cache of linear_activation_forward();\n",
        "    there are L of them, indexed from 0 to L-1\n",
        "    lambd -- regularization hyperparameter, scalar\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape)\n",
        "\n",
        "    # Lth layer (SOFTMAX -> LINEAR) gradients\n",
        "    linear_cache, activation_cache = caches[L-1]\n",
        "    A_prev, WL, bL = linear_cache\n",
        "    \n",
        "    dZL = AL - Y\n",
        "    dWL = 1./m * np.dot(dZL, A_prev.T) + ((lambd / m) * WL)\n",
        "    dbL = 1./m * np.sum(dZL, axis=1, keepdims = True)\n",
        "    dA_prev_temp = np.dot(WL.T, dZL)\n",
        "\n",
        "    grads[\"dW\" + str(L)] = dWL\n",
        "    grads[\"db\" + str(L)] = dbL\n",
        "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
        "\n",
        "    A = A_prev    \n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        linear_cache, activation_cache = caches[l]\n",
        "        A_prev, W, b = linear_cache\n",
        "\n",
        "        dZ = np.multiply(grads[\"dA\" + str(l + 1)], np.int64(A > 0))\n",
        "        dW_temp = 1./m * np.dot(dZ, A_prev.T) + ((lambd / m) * W)\n",
        "        db_temp = 1./m * np.sum(dZ, axis=1, keepdims = True)\n",
        "        dA_prev_temp = np.dot(W.T, dZ)\n",
        "        \n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "\n",
        "        A = A_prev\n",
        "        \n",
        "    return grads"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL0npPEqYh7I"
      },
      "source": [
        "# this function randomly generated minibatches from the entire training data\n",
        "\n",
        "def random_mini_batches(X, Y, mini_batch_size = 500):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, shape: (input size, number of examples)\n",
        "    Y -- one-hot encoded \"true\" label vector, shape: (number of classes, number of examples)\n",
        "    mini_batch_size -- size of the mini-batches, integer\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "        \n",
        "    # Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation]\n",
        "\n",
        "    # Partition (shuffled_X, shuffled_Y)\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9-ulhOohuFM"
      },
      "source": [
        "# this function initializes the velocity for use in gradient descent with momentum\n",
        "\n",
        "def initialize_velocity(parameters):\n",
        "    \"\"\"\n",
        "    Initializes the velocity as a dictionary with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros, same shape as the corresponding gradients/parameters\n",
        "    Arguments:\n",
        "    parameters -- dictionary containing model parameters\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    \n",
        "    Returns:\n",
        "    v -- dictionary containing the current velocity\n",
        "                    v['dW' + str(l)] = velocity of dWl\n",
        "                    v['db' + str(l)] = velocity of dbl\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    \n",
        "    # Initialize velocity\n",
        "    for l in range(L):\n",
        "        v[\"dW\" + str(l+1)] = np.zeros((parameters['W' + str(l+1)].shape[0], \n",
        "                                       parameters['W' + str(l+1)].shape[1]))\n",
        "        v[\"db\" + str(l+1)] = np.zeros((parameters['b' + str(l+1)].shape[0], \n",
        "                                       parameters['b' + str(l+1)].shape[1]))\n",
        "        \n",
        "    return v"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2emo_6Dij1z"
      },
      "source": [
        "# this function initializes first and second moments for use in Adam optimizer\n",
        "\n",
        "def initialize_adam(parameters) :\n",
        "    \"\"\"\n",
        "    Initializes v and s as two dictionaries with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros, same shape as the corresponding gradients/parameters\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- dictionary containing model parameters\n",
        "                    parameters[\"W\" + str(l)] = Wl\n",
        "                    parameters[\"b\" + str(l)] = bl\n",
        "    \n",
        "    Returns: \n",
        "    v -- dictionary that will contain the exponentially weighted average of the gradient\n",
        "                    v[\"dW\" + str(l)] = ...\n",
        "                    v[\"db\" + str(l)] = ...\n",
        "    s -- dictionary that will contain the exponentially weighted average of the squared gradient\n",
        "                    s[\"dW\" + str(l)] = ...\n",
        "                    s[\"db\" + str(l)] = ...\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # Initialize v, s\n",
        "    for l in range(L):\n",
        "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], \n",
        "                                       parameters[\"W\" + str(l+1)].shape[1]))\n",
        "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], \n",
        "                                       parameters[\"b\" + str(l+1)].shape[1]))\n",
        "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], \n",
        "                                       parameters[\"W\" + str(l+1)].shape[1]))\n",
        "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], \n",
        "                                       parameters[\"b\" + str(l+1)].shape[1]))\n",
        "    \n",
        "    return v, s"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iOrE6E3ZEF6"
      },
      "source": [
        "# this function updates model parameters using one step of gradient descent\n",
        "\n",
        "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using (minibatch) gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- dictionary containing model parameters\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- dictionary containing parameter gradients, output of L_model_backward\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    learning_rate -- determines step sizes during optimization, scalar\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- dictionary containing updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] \n",
        "        - (learning_rate * grads[\"dW\" + str(l+1)])\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] \n",
        "        - (learning_rate * grads[\"db\" + str(l+1)])\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEVML8xdjw7r"
      },
      "source": [
        "# this function updates model parameters when using momentum\n",
        "\n",
        "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using Momentum\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- dictionary containing model parameters\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- dictionary containing parameter gradients, output of L_model_backward\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- dictionary containing the current velocity\n",
        "                    v['dW' + str(l)] = ...\n",
        "                    v['db' + str(l)] = ...\n",
        "    beta -- the momentum hyperparameter, scalar\n",
        "    learning_rate -- determines step sizes during optimization, scalar\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- dictionary containing updated parameters \n",
        "    v -- dictionary containing updated velocities\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    \n",
        "    # Momentum update for each parameter\n",
        "    for l in range(L):\n",
        "        # compute velocities\n",
        "        v[\"dW\" + str(l+1)] = (beta * v[\"dW\" + str(l+1)]) + ((1.0 - beta) * grads[\"dW\" + str(l+1)])\n",
        "        v[\"db\" + str(l+1)] = (beta * v[\"db\" + str(l+1)]) + ((1.0 - beta) * grads[\"db\" + str(l+1)])\n",
        "        # update parameters\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * v[\"dW\" + str(l+1)])\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * v[\"db\" + str(l+1)])\n",
        "        \n",
        "    return parameters, v"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWfOb8lykK47"
      },
      "source": [
        "# this function updates model parameters when using Adam optimizer\n",
        "\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
        "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- dictionary containing model parameters\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- dictionary containing parameter gradients, output of L_model_backward\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- Adam variable, moving average of the first gradient, dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, dictionary\n",
        "    t -- counter required for Adam update, scalar\n",
        "    learning_rate -- determines step sizes during optimization, scalar\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "    Returns:\n",
        "    parameters -- dictionary containing updated parameters \n",
        "    v -- Adam variable, moving average of the first gradient, dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, dictionary\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "    \n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(L):\n",
        "        # Moving average of the gradients\n",
        "        v[\"dW\" + str(l+1)] = (beta1 * v[\"dW\" + str(l+1)]) + ((1.0 - beta1) * grads['dW' + str(l+1)])\n",
        "        v[\"db\" + str(l+1)] = (beta1 * v[\"db\" + str(l+1)]) + ((1.0 - beta1) * grads['db' + str(l+1)])\n",
        "\n",
        "        # Compute bias-corrected first moment estimate\n",
        "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1.0 - math.pow(beta1, t))\n",
        "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1.0 - math.pow(beta1, t))\n",
        "\n",
        "        # Moving average of the squared gradients\n",
        "        s[\"dW\" + str(l+1)] = \\\n",
        "            (beta2 * s[\"dW\" + str(l+1)]) + ((1.0 - beta2) * np.multiply(grads['dW' + str(l+1)], \n",
        "                                                                        grads['dW' + str(l+1)]))\n",
        "        s[\"db\" + str(l+1)] = \\\n",
        "            (beta2 * s[\"db\" + str(l+1)]) + ((1.0 - beta2) * np.multiply(grads['db' + str(l+1)], \n",
        "                                                                        grads['db' + str(l+1)]))\n",
        "\n",
        "        # Compute bias-corrected second raw moment estimate\n",
        "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1.0 - math.pow(beta2, t))\n",
        "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1.0 - math.pow(beta2, t))\n",
        "\n",
        "        # Update parameters\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n",
        "            (learning_rate * \n",
        "             (v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon)))\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
        "            (learning_rate * \n",
        "             (v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)))\n",
        "\n",
        "    return parameters, v, s"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sADd5dxfxb6"
      },
      "source": [
        "# this function implements the L-layer neural network model\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, optimizer=\"adam\", learning_rate = 0.0007, \n",
        "                  mini_batch_size = 500, beta = 0.9, beta1 = 0.9, beta2 = 0.999, \n",
        "                  epsilon = 1e-8, num_epochs = 10000, print_cost=False, \n",
        "                  initialization=\"he\", lambd = 0, keep_prob = 1.0):\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, shape: (input size, number of examples)\n",
        "    Y -- one-hot encoded \"true\" label vector, shape: (number of classes, number of examples)\n",
        "    layers_dims -- list containing number of hidden units for each layer\n",
        "    optimizer -- the optimizer to be used: \"gd\", \"momentum\" or \"adam\" \n",
        "    learning_rate -- determines step sizes during optimization, scalar\n",
        "    mini_batch_size -- the size of a mini batch\n",
        "    beta -- Momentum hyperparameter\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "    num_epochs -- number of passes over the training set during optimization\n",
        "    print_cost -- True to print the cost every 1000 epochs\n",
        "    initialization -- initialization to be used for model parameters: \"zeros\", \"random\" or \"he\"\n",
        "    lambd -- regularization hyperparameter, scalar\n",
        "    keep_prob -- probability of keeping a hidden unit active during drop-out, scalar\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model; used for making predictions\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []                         # keep track of cost\n",
        "    t = 0             # initializing the counter required for Adam update\n",
        "    m = X.shape[1]                   # number of training examples\n",
        "\n",
        "\n",
        "    # Initialize parameters\n",
        "    if initialization == \"zeros\":\n",
        "        parameters = initialize_parameters_zeros(layers_dims)\n",
        "    elif initialization == \"random\":\n",
        "        parameters = initialize_parameters_random(layers_dims)\n",
        "    elif initialization == \"he\":\n",
        "        parameters = initialize_parameters_he(layers_dims)\n",
        "    \n",
        "    # Initialize the optimizer\n",
        "    if optimizer == \"gd\":\n",
        "        pass # no initialization required for gradient descent\n",
        "    elif optimizer == \"momentum\":\n",
        "        v = initialize_velocity(parameters)\n",
        "    elif optimizer == \"adam\":\n",
        "        v, s = initialize_adam(parameters)\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in range(0, num_epochs):\n",
        "        minibatches = random_mini_batches(X, Y, mini_batch_size)\n",
        "        cost_total = 0\n",
        "\n",
        "        for minibatch in minibatches:\n",
        "\n",
        "            # Select a minibatch\n",
        "            (minibatch_X, minibatch_Y) = minibatch\n",
        "\n",
        "            # Forward propagation\n",
        "            AL, caches = L_model_forward(minibatch_X, parameters, keep_prob)\n",
        "        \n",
        "            # Cost function\n",
        "            if lambd == 0:\n",
        "                cost_total += compute_cost(AL, minibatch_Y)\n",
        "            else:\n",
        "                cost_total += compute_cost_with_regularization(AL, minibatch_Y, parameters, lambd)\n",
        "    \n",
        "            # Backward propagation.\n",
        "            if lambd == 0:\n",
        "                grads = L_model_backward(AL, minibatch_Y, caches, keep_prob)\n",
        "            else:\n",
        "                grads = L_model_backward_with_regularization(AL, minibatch_Y, caches, lambd)\n",
        "\n",
        "            # Update parameters\n",
        "            if optimizer == \"gd\":\n",
        "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
        "            elif optimizer == \"momentum\":\n",
        "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
        "            elif optimizer == \"adam\":\n",
        "                t = t + 1 # Adam counter\n",
        "                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
        "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
        "                \n",
        "        cost_avg = cost_total / m\n",
        "        \n",
        "        # Print the cost every 1000 epoch\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost_avg)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(costs)\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('epochs (per 100)')\n",
        "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L24EnlyhrbF",
        "outputId": "b9f5d214-ec10-4210-d8aa-aacc4751749c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# Loading the dataset\n",
        "\n",
        "filename = [\n",
        "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
        "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
        "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
        "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
        "]\n",
        "\n",
        "def download_mnist():\n",
        "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "    for name in filename:\n",
        "        print(\"Downloading \"+name[1])\n",
        "        request.urlretrieve(base_url+name[1], name[1])\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "def save_mnist():\n",
        "    mnist = {}\n",
        "    for name in filename[:2]:\n",
        "        with gzip.open(name[1], 'rb') as f:\n",
        "            temp_x = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28*28).T\n",
        "            mnist[name[0]] = temp_x.reshape(28, 28, temp_x.shape[1])\n",
        "    for name in filename[-2:]:\n",
        "        with gzip.open(name[1], 'rb') as f:\n",
        "            temp_y = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "            mnist[name[0]] = temp_y.reshape(1, temp_y.shape[0])\n",
        "    with open(\"mnist.pkl\", 'wb') as f:\n",
        "        pickle.dump(mnist,f)\n",
        "    print(\"Save complete.\")\n",
        "\n",
        "def load_dataset():\n",
        "    with open(\"mnist.pkl\",'rb') as f:\n",
        "        mnist = pickle.load(f)\n",
        "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
        "\n",
        "download_mnist()\n",
        "save_mnist()\n",
        "\n",
        "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig = load_dataset()\n",
        "num_classes = len(np.unique(Y_test_orig))\n",
        "\n",
        "print (\"X_train_orig shape: \" + str(X_train_orig.shape))\n",
        "print (\"Y_train_orig shape: \" + str(Y_train_orig.shape))\n",
        "print (\"X_test_orig shape: \" + str(X_test_orig.shape))\n",
        "print (\"Y_test_orig shape: \" + str(Y_test_orig.shape))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz\n",
            "Downloading t10k-images-idx3-ubyte.gz\n",
            "Downloading train-labels-idx1-ubyte.gz\n",
            "Downloading t10k-labels-idx1-ubyte.gz\n",
            "Download complete.\n",
            "Save complete.\n",
            "X_train_orig shape: (28, 28, 60000)\n",
            "Y_train_orig shape: (1, 60000)\n",
            "X_test_orig shape: (28, 28, 10000)\n",
            "Y_test_orig shape: (1, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIUo9VnaKa_7",
        "outputId": "b8763509-b705-42cc-a290-6f8067f343a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Example of a picture\n",
        "index = 7\n",
        "plt.imshow(X_train_orig[:,:,index])\n",
        "print (\"y = \" + str(np.squeeze(Y_train_orig[:,index])))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y = 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n02AixfXKTcG"
      },
      "source": [
        "# this function encodes true label vector with one-hot encoding\n",
        "\n",
        "def convert_to_one_hot(Y_raw, C):\n",
        "  \"\"\"\n",
        "    Encode true label vector with one-hot encoding\n",
        "    \n",
        "    Arguments:\n",
        "    Y_raw -- true label vector, size: (1, number of training examples)\n",
        "    C -- number of possible classes for labels\n",
        "\n",
        "    Returns:\n",
        "    Y -- one-hot encoded \"true\" label vector, shape: (number of classes, number of examples)\n",
        "  \"\"\"\n",
        "    \n",
        "  Y = np.eye(C)[Y_raw.reshape(-1)].T\n",
        "  return Y"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NtFJ2ipflgD",
        "outputId": "e0859039-0531-4a7c-fe3c-eba3e24c6f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Flatten the training and test images\n",
        "X_train_flatten = X_train_orig.reshape(\n",
        "    X_train_orig.shape[0]*X_train_orig.shape[1], X_train_orig.shape[2])\n",
        "X_test_flatten = X_test_orig.reshape(\n",
        "    X_test_orig.shape[0]*X_test_orig.shape[1], X_test_orig.shape[2])\n",
        "\n",
        "# Normalize image vectors\n",
        "X_train = X_train_flatten/255.\n",
        "X_test = X_test_flatten/255.\n",
        "\n",
        "# Convert training and test labels to one hot matrices\n",
        "Y_train = convert_to_one_hot(Y_train_orig, num_classes)\n",
        "Y_test = convert_to_one_hot(Y_test_orig, num_classes)\n",
        "\n",
        "print (\"number of label classes = \" + str(num_classes))\n",
        "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
        "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"Y_train shape: \" + str(Y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of label classes = 10\n",
            "number of training examples = 60000\n",
            "number of test examples = 10000\n",
            "X_train shape: (784, 60000)\n",
            "Y_train shape: (10, 60000)\n",
            "X_test shape: (784, 10000)\n",
            "Y_test shape: (10, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSsfJ4G3fqAy"
      },
      "source": [
        "# define number of hidden units in each layer\n",
        "\n",
        "layers_dims = [X_train.shape[0], 30, num_classes] #  2-layer model"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5bcywQ_f7pk",
        "outputId": "4bb2c705-c1ac-486f-fe68-853a8d911722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "# training the model\n",
        "\n",
        "# non regularized model\n",
        "# parameters = L_layer_model(X_train, Y_train, layers_dims, print_cost = True)\n",
        "\n",
        "# model with L2 regularization\n",
        "# parameters = L_layer_model(X_train, Y_train, layers_dims, print_cost = True, lambd = 0.7)\n",
        "\n",
        "# model with dropout\n",
        "parameters = L_layer_model(X_train, Y_train, layers_dims, print_cost = True, keep_prob = 0.86)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after epoch 0: 0.002375\n",
            "Cost after epoch 1000: 0.000088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after epoch 2000: 0.000079\n",
            "Cost after epoch 3000: 0.000071\n",
            "Cost after epoch 4000: 0.000073\n",
            "Cost after epoch 5000: 0.000072\n",
            "Cost after epoch 6000: 0.000066\n",
            "Cost after epoch 7000: 0.000065\n",
            "Cost after epoch 8000: 0.000063\n",
            "Cost after epoch 9000: 0.000063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jMhZjzwiCiy"
      },
      "source": [
        "# this function predicts the output for a validation/test set using the learnt neural network\n",
        "\n",
        "def predict(X, Y, parameters):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    Y -- one-hot encoded \"true\" label vector\n",
        "    parameters -- parameters of the trained model\n",
        "    \n",
        "    Returns:\n",
        "    p -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    p = np.zeros((1,m))\n",
        "    y = np.zeros((1,m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_model_forward(X, parameters)\n",
        "\n",
        "    \n",
        "    # convert probas to one of 0....k predicted labels\n",
        "    for i in range(0, probas.shape[1]):\n",
        "        p[0,i] = np.argmax(probas[:,i])\n",
        "        y[0,i] = np.argmax(Y[:,i])\n",
        "      \n",
        "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
        "        \n",
        "    return p"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zu2VdJff_9u",
        "outputId": "c712b7b9-e0b8-4155-82d4-88dfe4e400bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# training accuracy\n",
        "\n",
        "pred_train = predict(X_train, Y_train, parameters)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9995666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq6e6lpegEZP",
        "outputId": "1b652d58-eca7-4437-fccf-a98944f7623b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# validation/test accuracy\n",
        "\n",
        "pred_test = predict(X_test, Y_test, parameters)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9459000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjR5zXTTNIOP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}